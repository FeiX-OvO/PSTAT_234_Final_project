{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\n",
    "from os.path import join\n",
    "import abc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from layer import cal_metric\n",
    "from layer import SelfAttention\n",
    "\n",
    "from layer import (\n",
    "    AttLayer2,\n",
    "    ComputeMasking,\n",
    "    OverwriteMasking,\n",
    ")\n",
    "\n",
    "class BaseModel:\n",
    "    \"\"\"Basic class of models\n",
    "\n",
    "    Attributes:\n",
    "        hparams (obj): A tf.contrib.training.HParams object, hold the entire set of hyperparameters.\n",
    "        iterator_creator_train (obj): An iterator to load the data in training steps.\n",
    "        iterator_creator_train (obj): An iterator to load the data in testing steps.\n",
    "        graph (obj): An optional graph.\n",
    "        seed (int): Random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams,\n",
    "        iterator_creator,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Initializing the model. Create common logics which are needed by all deeprec models, such as loss function,\n",
    "        parameter set.\n",
    "\n",
    "        Args:\n",
    "            hparams (obj): A tf.contrib.training.HParams object, hold the entire set of hyperparameters.\n",
    "            iterator_creator_train (obj): An iterator to load the data in training steps.\n",
    "            iterator_creator_train (obj): An iterator to load the data in testing steps.\n",
    "            graph (obj): An optional graph.\n",
    "            seed (int): Random seed.\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.train_iterator = iterator_creator(\n",
    "            hparams,\n",
    "            hparams.npratio,\n",
    "            col_spliter=\"\\t\",\n",
    "        )\n",
    "        self.test_iterator = iterator_creator(\n",
    "            hparams,\n",
    "            col_spliter=\"\\t\",\n",
    "        )\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.support_quick_scoring = hparams.support_quick_scoring\n",
    "\n",
    "        # set GPU use with on demand growth\n",
    "        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "        sess = tf.compat.v1.Session(\n",
    "            config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "        )\n",
    "\n",
    "        # set this TensorFlow session as the default session for Keras\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "        # IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras!\n",
    "        # Otherwise, their weights will be unavailable in the threads after the session there has been set\n",
    "        self.model, self.scorer = self._build_graph()\n",
    "\n",
    "        self.loss = self._get_loss()\n",
    "        self.train_optimizer = self._get_opt()\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.train_optimizer)\n",
    "\n",
    "    def _init_embedding(self, file_path):\n",
    "        \"\"\"Load pre-trained embeddings as a constant tensor.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): the pre-trained glove embeddings file path.\n",
    "\n",
    "        Returns:\n",
    "            np.array: A constant numpy array.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.load(file_path)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Subclass will implement this.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _get_input_label_from_iter(self, batch_data):\n",
    "        \"\"\"Subclass will implement this\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_loss(self):\n",
    "        \"\"\"Make loss function, consists of data loss and regularization loss\n",
    "\n",
    "        Returns:\n",
    "            obj: Loss function or loss function name\n",
    "        \"\"\"\n",
    "        if self.hparams.loss == \"cross_entropy_loss\":\n",
    "            data_loss = \"categorical_crossentropy\"\n",
    "        elif self.hparams.loss == \"log_loss\":\n",
    "            data_loss = \"binary_crossentropy\"\n",
    "        else:\n",
    "            raise ValueError(\"this loss not defined {0}\".format(self.hparams.loss))\n",
    "        return data_loss\n",
    "\n",
    "    def _get_opt(self):\n",
    "        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "        Returns:\n",
    "            obj: An optimizer.\n",
    "        \"\"\"\n",
    "        lr = self.hparams.learning_rate\n",
    "        optimizer = self.hparams.optimizer\n",
    "\n",
    "        if optimizer == \"adam\":\n",
    "            train_opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "        return train_opt\n",
    "\n",
    "    def _get_pred(self, logit, task):\n",
    "        \"\"\"Make final output as prediction score, according to different tasks.\n",
    "\n",
    "        Args:\n",
    "            logit (obj): Base prediction value.\n",
    "            task (str): A task (values: regression/classification)\n",
    "\n",
    "        Returns:\n",
    "            obj: Transformed score\n",
    "        \"\"\"\n",
    "        if task == \"regression\":\n",
    "            pred = tf.identity(logit)\n",
    "        elif task == \"classification\":\n",
    "            pred = tf.sigmoid(logit)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"method must be regression or classification, but now is {0}\".format(\n",
    "                    task\n",
    "                )\n",
    "            )\n",
    "        return pred\n",
    "\n",
    "    def train(self, train_batch_data):\n",
    "        \"\"\"Go through the optimization step once with training data in feed_dict.\n",
    "\n",
    "        Args:\n",
    "            sess (obj): The model session object.\n",
    "            feed_dict (dict): Feed values to train the model. This is a dictionary that maps graph elements to values.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of values, including update operation, total loss, data loss, and merged summary.\n",
    "        \"\"\"\n",
    "        train_input, train_label = self._get_input_label_from_iter(train_batch_data)\n",
    "        rslt = self.model.train_on_batch(train_input, train_label)\n",
    "        return rslt\n",
    "\n",
    "    def eval(self, eval_batch_data):\n",
    "        \"\"\"Evaluate the data in feed_dict with current model.\n",
    "\n",
    "        Args:\n",
    "            sess (obj): The model session object.\n",
    "            feed_dict (dict): Feed values for evaluation. This is a dictionary that maps graph elements to values.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of evaluated results, including total loss value, data loss value,\n",
    "                predicted scores, and ground-truth labels.\n",
    "        \"\"\"\n",
    "        eval_input, eval_label = self._get_input_label_from_iter(eval_batch_data)\n",
    "        imp_index = eval_batch_data[\"impression_index_batch\"]\n",
    "\n",
    "        pred_rslt = self.scorer.predict_on_batch(eval_input)\n",
    "\n",
    "        return pred_rslt, eval_label, imp_index\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_news_file,\n",
    "        train_behaviors_file,\n",
    "        valid_news_file,\n",
    "        valid_behaviors_file,\n",
    "        test_news_file=None,\n",
    "        test_behaviors_file=None,\n",
    "    ):\n",
    "        \"\"\"Fit the model with train_file. Evaluate the model on valid_file per epoch to observe the training status.\n",
    "        If test_news_file is not None, evaluate it too.\n",
    "\n",
    "        Args:\n",
    "            train_file (str): training data set.\n",
    "            valid_file (str): validation set.\n",
    "            test_news_file (str): test set.\n",
    "\n",
    "        Returns:\n",
    "            obj: An instance of self.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_losses=[]\n",
    "        val_losses=[]\n",
    "        val_result=[]\n",
    "        for epoch in range(1, self.hparams.epochs + 1):\n",
    "            step = 0\n",
    "            self.hparams.current_epoch = epoch\n",
    "            epoch_loss = 0\n",
    "            train_start = time.time()\n",
    "\n",
    "            tqdm_util = tqdm(\n",
    "                self.train_iterator.load_data_from_file(\n",
    "                    train_news_file, train_behaviors_file\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for batch_data_input in tqdm_util:\n",
    "\n",
    "                step_result = self.train(batch_data_input)\n",
    "                step_data_loss = step_result\n",
    "\n",
    "                epoch_loss += step_data_loss\n",
    "                step += 1\n",
    "                if step % self.hparams.show_step == 0:\n",
    "                    tqdm_util.set_description(\n",
    "                        \"step {0:d} , total_loss: {1:.4f}, data_loss: {2:.4f}\".format(\n",
    "                            step, epoch_loss / step, step_data_loss\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "            train_losses.append(epoch_loss/step)\n",
    "            train_end = time.time()\n",
    "            train_time = train_end - train_start\n",
    "\n",
    "            eval_start = time.time()\n",
    "\n",
    "            train_info = \",\".join(\n",
    "                [\n",
    "                    str(item[0]) + \":\" + str(item[1])\n",
    "                    for item in [(\"logloss loss\", epoch_loss / step)]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            eval_res = self.run_eval(valid_news_file, valid_behaviors_file)\n",
    "            \n",
    "            val_result.append(eval_res)\n",
    "            \n",
    "            eval_info = \", \".join(\n",
    "                [\n",
    "                    str(item[0]) + \":\" + str(item[1])\n",
    "                    for item in sorted(eval_res.items(), key=lambda x: x[0])\n",
    "                ]\n",
    "            )\n",
    "            if test_news_file is not None:\n",
    "                test_res = self.run_eval(test_news_file, test_behaviors_file)\n",
    "                test_info = \", \".join(\n",
    "                    [\n",
    "                        str(item[0]) + \":\" + str(item[1])\n",
    "                        for item in sorted(test_res.items(), key=lambda x: x[0])\n",
    "                    ]\n",
    "                )\n",
    "            eval_end = time.time()\n",
    "            eval_time = eval_end - eval_start\n",
    "\n",
    "            if test_news_file is not None:\n",
    "                print(\n",
    "                    \"at epoch {0:d}\".format(epoch)\n",
    "                    + \"\\ntrain info: \"\n",
    "                    + train_info\n",
    "                    + \"\\neval info: \"\n",
    "                    + eval_info\n",
    "                    + \"\\ntest info: \"\n",
    "                    + test_info\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"at epoch {0:d}\".format(epoch)\n",
    "                    + \"\\ntrain info: \"\n",
    "                    + train_info\n",
    "                    + \"\\neval info: \"\n",
    "                    + eval_info\n",
    "                )\n",
    "            print(\n",
    "                \"at epoch {0:d} , train time: {1:.1f} eval time: {2:.1f}\".format(\n",
    "                    epoch, train_time, eval_time\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if epoch%5==0:\n",
    "                self.model.save_weights(os.path.join('data/model_newlr', \"lstur_lr3e-4_{}\".format(epoch)))\n",
    "        return train_losses, val_result\n",
    "\n",
    "    def group_labels(self, labels, preds, group_keys):\n",
    "        \"\"\"Devide labels and preds into several group according to values in group keys.\n",
    "\n",
    "        Args:\n",
    "            labels (list): ground truth label list.\n",
    "            preds (list): prediction score list.\n",
    "            group_keys (list): group key list.\n",
    "\n",
    "        Returns:\n",
    "            all_labels: labels after group.\n",
    "            all_preds: preds after group.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        all_keys = list(set(group_keys))\n",
    "        all_keys.sort()\n",
    "        group_labels = {k: [] for k in all_keys}\n",
    "        group_preds = {k: [] for k in all_keys}\n",
    "\n",
    "        for l, p, k in zip(labels, preds, group_keys):\n",
    "            group_labels[k].append(l)\n",
    "            group_preds[k].append(p)\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for k in all_keys:\n",
    "            all_labels.append(group_labels[k])\n",
    "            all_preds.append(group_preds[k])\n",
    "\n",
    "        return all_keys, all_labels, all_preds\n",
    "\n",
    "    def run_eval(self, news_filename, behaviors_file):\n",
    "        \"\"\"Evaluate the given file and returns some evaluation metrics.\n",
    "\n",
    "        Args:\n",
    "            filename (str): A file name that will be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary contains evaluation metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.support_quick_scoring:\n",
    "            _, group_labels, group_preds = self.run_fast_eval(\n",
    "                news_filename, behaviors_file\n",
    "            )\n",
    "        else:\n",
    "            _, group_labels, group_preds = self.run_slow_eval(\n",
    "                news_filename, behaviors_file\n",
    "            )\n",
    "        res = cal_metric(group_labels, group_preds, self.hparams.metrics)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def user(self, batch_user_input):\n",
    "        user_input = self._get_user_feature_from_iter(batch_user_input)\n",
    "        user_vec = self.userencoder.predict_on_batch(user_input)\n",
    "        user_index = batch_user_input[\"impr_index_batch\"]\n",
    "\n",
    "        return user_index, user_vec\n",
    "\n",
    "    def news(self, batch_news_input):\n",
    "        news_input = self._get_news_feature_from_iter(batch_news_input)\n",
    "        news_vec = self.newsencoder.predict_on_batch(news_input)\n",
    "        news_index = batch_news_input[\"news_index_batch\"]\n",
    "\n",
    "        return news_index, news_vec\n",
    "\n",
    "    def run_user(self, news_filename, behaviors_file):\n",
    "        if not hasattr(self, \"userencoder\"):\n",
    "            raise ValueError(\"model must have attribute userencoder\")\n",
    "\n",
    "        user_indexes = []\n",
    "        user_vecs = []\n",
    "        for batch_data_input in tqdm(\n",
    "            self.test_iterator.load_user_from_file(news_filename, behaviors_file)\n",
    "        ):\n",
    "            user_index, user_vec = self.user(batch_data_input)\n",
    "            user_indexes.extend(np.reshape(user_index, -1))\n",
    "            user_vecs.extend(user_vec)\n",
    "\n",
    "        return dict(zip(user_indexes, user_vecs))\n",
    "\n",
    "    def run_news(self, news_filename):\n",
    "        if not hasattr(self, \"newsencoder\"):\n",
    "            raise ValueError(\"model must have attribute newsencoder\")\n",
    "\n",
    "        news_indexes = []\n",
    "        news_vecs = []\n",
    "        for batch_data_input in tqdm(\n",
    "            self.test_iterator.load_news_from_file(news_filename)\n",
    "        ):\n",
    "            news_index, news_vec = self.news(batch_data_input)\n",
    "            news_indexes.extend(np.reshape(news_index, -1))\n",
    "            news_vecs.extend(news_vec)\n",
    "\n",
    "        return dict(zip(news_indexes, news_vecs))\n",
    "\n",
    "    def run_slow_eval(self, news_filename, behaviors_file):\n",
    "        preds = []\n",
    "        labels = []\n",
    "        imp_indexes = []\n",
    "\n",
    "        for batch_data_input in tqdm(\n",
    "            self.test_iterator.load_data_from_file(news_filename, behaviors_file)\n",
    "        ):\n",
    "            step_pred, step_labels, step_imp_index = self.eval(batch_data_input)\n",
    "            preds.extend(np.reshape(step_pred, -1))\n",
    "            labels.extend(np.reshape(step_labels, -1))\n",
    "            imp_indexes.extend(np.reshape(step_imp_index, -1))\n",
    "\n",
    "        group_impr_indexes, group_labels, group_preds = self.group_labels(\n",
    "            labels, preds, imp_indexes\n",
    "        )\n",
    "        return group_impr_indexes, group_labels, group_preds\n",
    "\n",
    "    def run_fast_eval(self, news_filename, behaviors_file):\n",
    "        news_vecs = self.run_news(news_filename)\n",
    "        user_vecs = self.run_user(news_filename, behaviors_file)\n",
    "\n",
    "        self.news_vecs = news_vecs\n",
    "        self.user_vecs = user_vecs\n",
    "\n",
    "        group_impr_indexes = []\n",
    "        group_labels = []\n",
    "        group_preds = []\n",
    "\n",
    "        for (\n",
    "            impr_index,\n",
    "            news_index,\n",
    "            user_index,\n",
    "            label,\n",
    "        ) in tqdm(self.test_iterator.load_impression_from_file(behaviors_file)):\n",
    "            pred = np.dot(\n",
    "                np.stack([news_vecs[i] for i in news_index], axis=0),\n",
    "                user_vecs[impr_index],\n",
    "            )\n",
    "            group_impr_indexes.append(impr_index)\n",
    "            group_labels.append(label)\n",
    "            group_preds.append(pred)\n",
    "\n",
    "        return group_impr_indexes, group_labels, group_preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTURModel(BaseModel):\n",
    "    \"\"\"LSTUR model(Neural News Recommendation with Multi-Head Self-Attention)\n",
    "\n",
    "    Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie: \n",
    "    Neural News Recommendation with Long- and Short-term User Representations, ACL 2019\n",
    "\n",
    "    Attributes:\n",
    "        word2vec_embedding (numpy.array): Pretrained word embedding matrix.\n",
    "        hparam (obj): Global hyper-parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, iterator_creator, seed=None):\n",
    "        \"\"\"Initialization steps for LSTUR.\n",
    "        Compared with the BaseModel, LSTUR need word embedding.\n",
    "        After creating word embedding matrix, BaseModel's __init__ method will be called.\n",
    "        \n",
    "        Args:\n",
    "            hparams (obj): Global hyper-parameters. Some key setttings such as type and gru_unit are there.\n",
    "            iterator_creator_train(obj): LSTUR data loader class for train data.\n",
    "            iterator_creator_test(obj): LSTUR data loader class for test and validation data\n",
    "        \"\"\"\n",
    "\n",
    "        self.word2vec_embedding = self._init_embedding(hparams.wordEmb_file)\n",
    "        self.hparam = hparams\n",
    "\n",
    "        super().__init__(hparams, iterator_creator, seed=seed)\n",
    "\n",
    "    def _get_input_label_from_iter(self, batch_data):\n",
    "        input_feat = [\n",
    "            batch_data[\"user_index_batch\"],\n",
    "            batch_data[\"clicked_title_batch\"],\n",
    "            batch_data[\"candidate_title_batch\"],\n",
    "        ]\n",
    "        input_label = batch_data[\"labels\"]\n",
    "        return input_feat, input_label\n",
    "\n",
    "    def _get_user_feature_from_iter(self, batch_data):\n",
    "        return [batch_data[\"clicked_title_batch\"], batch_data[\"user_index_batch\"]]\n",
    "\n",
    "    def _get_news_feature_from_iter(self, batch_data):\n",
    "        return batch_data[\"candidate_title_batch\"]\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build LSTUR model and scorer.\n",
    "\n",
    "        Returns:\n",
    "            obj: a model used to train.\n",
    "            obj: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "\n",
    "        model, scorer = self._build_lstur()\n",
    "        return model, scorer\n",
    "\n",
    "    def _build_userencoder(self, titleencoder, type=\"ini\"):\n",
    "        \"\"\"The main function to create user encoder of LSTUR.\n",
    "\n",
    "        Args:\n",
    "            titleencoder(obj): the news encoder of LSTUR. \n",
    "\n",
    "        Return:\n",
    "            obj: the user encoder of LSTUR.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        his_input_title = keras.Input(\n",
    "            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n",
    "        )\n",
    "        user_indexes = keras.Input(shape=(1,), dtype=\"int32\")\n",
    "\n",
    "        user_embedding_layer = layers.Embedding(\n",
    "            len(self.train_iterator.uid2index),\n",
    "            hparams.gru_unit,\n",
    "            trainable=True,\n",
    "            embeddings_initializer=\"zeros\",\n",
    "        )\n",
    "\n",
    "        long_u_emb = layers.Reshape((hparams.gru_unit,))(\n",
    "            user_embedding_layer(user_indexes)\n",
    "        )\n",
    "        click_title_presents = layers.TimeDistributed(titleencoder)(his_input_title)\n",
    "\n",
    "        if type == \"ini\":\n",
    "            user_present = layers.GRU(\n",
    "                hparams.gru_unit,\n",
    "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "                recurrent_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "                bias_initializer=keras.initializers.Zeros(),\n",
    "            )(\n",
    "                layers.Masking(mask_value=0.0)(click_title_presents),\n",
    "                initial_state=[long_u_emb],\n",
    "            )\n",
    "        elif type == \"con\":\n",
    "            short_uemb = layers.GRU(\n",
    "                hparams.gru_unit,\n",
    "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "                recurrent_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "                bias_initializer=keras.initializers.Zeros(),\n",
    "            )(layers.Masking(mask_value=0.0)(click_title_presents))\n",
    "\n",
    "            user_present = layers.Concatenate()([short_uemb, long_u_emb])\n",
    "            user_present = layers.Dense(\n",
    "                hparams.gru_unit,\n",
    "                bias_initializer=keras.initializers.Zeros(),\n",
    "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            )(user_present)\n",
    "\n",
    "        model = keras.Model(\n",
    "            [his_input_title, user_indexes], user_present, name=\"user_encoder\"\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def _build_newsencoder(self, embedding_layer):\n",
    "        \"\"\"The main function to create news encoder of LSTUR.\n",
    "\n",
    "        Args:\n",
    "            embedding_layer(obj): a word embedding layer.\n",
    "        \n",
    "        Return:\n",
    "            obj: the news encoder of LSTUR.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        sequences_input_title = keras.Input(shape=(hparams.title_size,), dtype=\"int32\")\n",
    "        embedded_sequences_title = embedding_layer(sequences_input_title)\n",
    "\n",
    "        y = layers.Dropout(hparams.dropout)(embedded_sequences_title)\n",
    "        y = layers.Conv1D(\n",
    "            hparams.filter_num,\n",
    "            hparams.window_size,\n",
    "            activation=hparams.cnn_activation,\n",
    "            padding=\"same\",\n",
    "            bias_initializer=keras.initializers.Zeros(),\n",
    "            kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "        )(y)\n",
    "        print(y)\n",
    "        y = layers.Dropout(hparams.dropout)(y)\n",
    "        y = layers.Masking()(\n",
    "            OverwriteMasking()([y, ComputeMasking()(sequences_input_title)])\n",
    "        )\n",
    "        pred_title = AttLayer2(hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "        print(pred_title)\n",
    "        model = keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
    "        return model\n",
    "\n",
    "    def _build_lstur(self):\n",
    "        \"\"\"The main function to create LSTUR's logic. The core of LSTUR\n",
    "        is a user encoder and a news encoder.\n",
    "        \n",
    "        Returns:\n",
    "            obj: a model used to train.\n",
    "            obj: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "\n",
    "        his_input_title = keras.Input(\n",
    "            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n",
    "        )\n",
    "        pred_input_title = keras.Input(\n",
    "            shape=(hparams.npratio + 1, hparams.title_size), dtype=\"int32\"\n",
    "        )\n",
    "        pred_input_title_one = keras.Input(\n",
    "            shape=(1, hparams.title_size,), dtype=\"int32\"\n",
    "        )\n",
    "        pred_title_reshape = layers.Reshape((hparams.title_size,))(pred_input_title_one)\n",
    "        user_indexes = keras.Input(shape=(1,), dtype=\"int32\")\n",
    "\n",
    "        embedding_layer = layers.Embedding(\n",
    "            self.word2vec_embedding.shape[0],\n",
    "            hparams.word_emb_dim,\n",
    "            weights=[self.word2vec_embedding],\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        titleencoder = self._build_newsencoder(embedding_layer)\n",
    "        self.userencoder = self._build_userencoder(titleencoder, type=hparams.type)\n",
    "        self.newsencoder = titleencoder\n",
    "\n",
    "        user_present = self.userencoder([his_input_title, user_indexes])\n",
    "        news_present = layers.TimeDistributed(self.newsencoder)(pred_input_title)\n",
    "        news_present_one = self.newsencoder(pred_title_reshape)\n",
    "\n",
    "        preds = layers.Dot(axes=-1)([news_present, user_present])\n",
    "        preds = layers.Activation(activation=\"softmax\")(preds)\n",
    "\n",
    "        pred_one = layers.Dot(axes=-1)([news_present_one, user_present])\n",
    "        pred_one = layers.Activation(activation=\"sigmoid\")(pred_one)\n",
    "\n",
    "        model = keras.Model([user_indexes, his_input_title, pred_input_title], preds)\n",
    "        scorer = keras.Model(\n",
    "            [user_indexes, his_input_title, pred_input_title_one], pred_one\n",
    "        )\n",
    "\n",
    "        return model, scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "# import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from utils import download_deeprec_resources \n",
    "from utils import prepare_hparams\n",
    "from utils import get_mind_data_set\n",
    "from iterator import MINDIterator\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "seed = 40\n",
    "batch_size = 32\n",
    "\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmpdir = TemporaryDirectory()\n",
    "# data_path = tmpdir.name\n",
    "data_path='data'\n",
    "\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding_all.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict_all.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'lstur.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_format=news,iterator_type=None,support_quick_scoring=True,wordEmb_file=data/utils/embedding_all.npy,wordDict_file=data/utils/word_dict_all.pkl,userDict_file=data/utils/uid2index.pkl,vertDict_file=None,subvertDict_file=None,title_size=30,body_size=None,word_emb_dim=300,word_size=None,user_num=None,vert_num=None,subvert_num=None,his_size=50,npratio=4,dropout=0.2,attention_hidden_dim=200,head_num=4,head_dim=100,cnn_activation=relu,dense_activation=None,filter_num=400,window_size=3,vert_emb_dim=100,subvert_emb_dim=100,gru_unit=400,type=ini,user_emb_dim=50,learning_rate=0.0003,loss=cross_entropy_loss,optimizer=adam,epochs=50,batch_size=32,show_step=100000,metrics=['group_auc', 'mean_mrr', 'ndcg@5;10']\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "\n",
    "hparams.learning_rate=3e-4\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d/Relu:0\", shape=(?, 30, 400), dtype=float32)\n",
      "Tensor(\"att_layer2/Sum_1:0\", shape=(?, 400), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "iterator = MINDIterator\n",
    "model = LSTURModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:15,  5.54it/s]\n",
      "586it [00:02, 238.23it/s]\n",
      "236it [00:10, 21.47it/s]\n",
      "7538it [00:02, 2562.42it/s]\n",
      "1it [00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.4713537685339841\n",
      "eval info: group_auc:0.6127, mean_mrr:0.2745, ndcg@10:0.3648, ndcg@5:0.3004\n",
      "at epoch 1 , train time: 195.9 eval time: 25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:02,  5.97it/s]\n",
      "586it [00:01, 356.59it/s]\n",
      "236it [00:09, 24.01it/s]\n",
      "7538it [00:02, 2848.88it/s]\n",
      "1it [00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.367838222732421\n",
      "eval info: group_auc:0.6317, mean_mrr:0.2906, ndcg@10:0.3801, ndcg@5:0.3175\n",
      "at epoch 2 , train time: 182.0 eval time: 23.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.98it/s]\n",
      "586it [00:01, 344.45it/s]\n",
      "236it [00:09, 23.85it/s]\n",
      "7538it [00:02, 2871.67it/s]\n",
      "1it [00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.2906187642125575\n",
      "eval info: group_auc:0.6467, mean_mrr:0.2962, ndcg@10:0.391, ndcg@5:0.3271\n",
      "at epoch 3 , train time: 181.6 eval time: 22.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.97it/s]\n",
      "586it [00:01, 345.79it/s]\n",
      "236it [00:09, 23.97it/s]\n",
      "7538it [00:02, 3391.10it/s]\n",
      "1it [00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.2036339394185644\n",
      "eval info: group_auc:0.6268, mean_mrr:0.2839, ndcg@10:0.3751, ndcg@5:0.3095\n",
      "at epoch 4 , train time: 182.0 eval time: 23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:59,  6.06it/s]\n",
      "586it [00:01, 329.14it/s]\n",
      "236it [00:09, 24.09it/s]\n",
      "7538it [00:02, 3374.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 5\n",
      "train info: logloss loss:1.1289678005223776\n",
      "eval info: group_auc:0.6394, mean_mrr:0.2955, ndcg@10:0.3883, ndcg@5:0.3205\n",
      "at epoch 5 , train time: 179.1 eval time: 23.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:59,  6.06it/s]\n",
      "586it [00:01, 320.87it/s]\n",
      "236it [00:09, 24.05it/s]\n",
      "7538it [00:02, 3243.25it/s]\n",
      "1it [00:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 6\n",
      "train info: logloss loss:1.0540410205584643\n",
      "eval info: group_auc:0.6336, mean_mrr:0.2918, ndcg@10:0.3832, ndcg@5:0.3159\n",
      "at epoch 6 , train time: 179.3 eval time: 22.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:57,  6.10it/s]\n",
      "586it [00:01, 320.82it/s]\n",
      "236it [00:09, 24.39it/s]\n",
      "7538it [00:02, 3294.87it/s]\n",
      "1it [00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 7\n",
      "train info: logloss loss:0.9804196330833611\n",
      "eval info: group_auc:0.6294, mean_mrr:0.2893, ndcg@10:0.3814, ndcg@5:0.3162\n",
      "at epoch 7 , train time: 178.0 eval time: 22.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:59,  6.04it/s]\n",
      "586it [00:01, 364.29it/s]\n",
      "236it [00:09, 23.96it/s]\n",
      "7538it [00:02, 2705.33it/s]\n",
      "1it [00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 8\n",
      "train info: logloss loss:0.8981508883488113\n",
      "eval info: group_auc:0.6237, mean_mrr:0.2838, ndcg@10:0.3722, ndcg@5:0.3067\n",
      "at epoch 8 , train time: 179.8 eval time: 23.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:00,  6.03it/s]\n",
      "586it [00:01, 326.32it/s]\n",
      "236it [00:09, 24.31it/s]\n",
      "7538it [00:01, 3943.09it/s]\n",
      "1it [00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 9\n",
      "train info: logloss loss:0.8255947938610836\n",
      "eval info: group_auc:0.6358, mean_mrr:0.2936, ndcg@10:0.385, ndcg@5:0.32\n",
      "at epoch 9 , train time: 180.0 eval time: 22.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:57,  6.12it/s]\n",
      "586it [00:01, 350.01it/s]\n",
      "236it [00:09, 24.19it/s]\n",
      "7538it [00:01, 3981.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 10\n",
      "train info: logloss loss:0.7465287481004143\n",
      "eval info: group_auc:0.6298, mean_mrr:0.2875, ndcg@10:0.3785, ndcg@5:0.3157\n",
      "at epoch 10 , train time: 177.4 eval time: 22.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:56,  6.15it/s]\n",
      "586it [00:01, 323.66it/s]\n",
      "236it [00:09, 24.43it/s]\n",
      "7538it [00:01, 4041.43it/s]\n",
      "1it [00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 11\n",
      "train info: logloss loss:0.6813034349657993\n",
      "eval info: group_auc:0.6234, mean_mrr:0.2829, ndcg@10:0.3717, ndcg@5:0.3079\n",
      "at epoch 11 , train time: 176.7 eval time: 22.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:57,  6.12it/s]\n",
      "586it [00:01, 309.30it/s]\n",
      "236it [00:09, 24.31it/s]\n",
      "7538it [00:01, 4011.44it/s]\n",
      "1it [00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 12\n",
      "train info: logloss loss:0.6245043806827749\n",
      "eval info: group_auc:0.6234, mean_mrr:0.2851, ndcg@10:0.3741, ndcg@5:0.3083\n",
      "at epoch 12 , train time: 177.5 eval time: 22.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:57,  6.12it/s]\n",
      "586it [00:01, 339.47it/s]\n",
      "236it [00:09, 24.28it/s]\n",
      "7538it [00:01, 3916.82it/s]\n",
      "1it [00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 13\n",
      "train info: logloss loss:0.56537083806693\n",
      "eval info: group_auc:0.619, mean_mrr:0.284, ndcg@10:0.373, ndcg@5:0.3075\n",
      "at epoch 13 , train time: 177.5 eval time: 22.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:00,  6.02it/s]\n",
      "586it [00:01, 323.16it/s]\n",
      "236it [00:09, 23.89it/s]\n",
      "7538it [00:02, 2778.86it/s]\n",
      "1it [00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 14\n",
      "train info: logloss loss:0.5183553358687442\n",
      "eval info: group_auc:0.6252, mean_mrr:0.2859, ndcg@10:0.3756, ndcg@5:0.311\n",
      "at epoch 14 , train time: 180.3 eval time: 22.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.98it/s]\n",
      "586it [00:01, 331.55it/s]\n",
      "236it [00:09, 23.92it/s]\n",
      "7538it [00:01, 4421.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 15\n",
      "train info: logloss loss:0.47882760726395673\n",
      "eval info: group_auc:0.6263, mean_mrr:0.2857, ndcg@10:0.3738, ndcg@5:0.31\n",
      "at epoch 15 , train time: 181.6 eval time: 22.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.99it/s]\n",
      "586it [00:01, 337.95it/s]\n",
      "236it [00:09, 23.98it/s]\n",
      "7538it [00:02, 2914.47it/s]\n",
      "1it [00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 16\n",
      "train info: logloss loss:0.4328786233031838\n",
      "eval info: group_auc:0.6208, mean_mrr:0.2839, ndcg@10:0.3734, ndcg@5:0.3084\n",
      "at epoch 16 , train time: 181.3 eval time: 22.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:58,  6.07it/s]\n",
      "586it [00:01, 323.23it/s]\n",
      "236it [00:09, 24.01it/s]\n",
      "7538it [00:02, 3426.91it/s]\n",
      "1it [00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 17\n",
      "train info: logloss loss:0.3988796986200423\n",
      "eval info: group_auc:0.6233, mean_mrr:0.2881, ndcg@10:0.3765, ndcg@5:0.3128\n",
      "at epoch 17 , train time: 178.8 eval time: 23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [02:59,  6.06it/s]\n",
      "586it [00:01, 340.22it/s]\n",
      "236it [00:09, 24.12it/s]\n",
      "7538it [00:02, 3576.28it/s]\n",
      "1it [00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 18\n",
      "train info: logloss loss:0.37780462335074805\n",
      "eval info: group_auc:0.6237, mean_mrr:0.2873, ndcg@10:0.3748, ndcg@5:0.3121\n",
      "at epoch 18 , train time: 179.1 eval time: 22.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  6.00it/s]\n",
      "586it [00:01, 341.82it/s]\n",
      "236it [00:09, 24.10it/s]\n",
      "7538it [00:02, 2748.08it/s]\n",
      "1it [00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 19\n",
      "train info: logloss loss:0.34994708079138076\n",
      "eval info: group_auc:0.6198, mean_mrr:0.285, ndcg@10:0.3727, ndcg@5:0.3091\n",
      "at epoch 19 , train time: 181.0 eval time: 22.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.99it/s]\n",
      "586it [00:01, 330.71it/s]\n",
      "236it [00:09, 23.76it/s]\n",
      "7538it [00:01, 3867.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 20\n",
      "train info: logloss loss:0.32976444670673233\n",
      "eval info: group_auc:0.6204, mean_mrr:0.2862, ndcg@10:0.3743, ndcg@5:0.3111\n",
      "at epoch 20 , train time: 181.4 eval time: 23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:02,  5.95it/s]\n",
      "586it [00:01, 421.74it/s]\n",
      "236it [00:09, 24.29it/s]\n",
      "7538it [00:02, 2613.10it/s]\n",
      "1it [00:00,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 21\n",
      "train info: logloss loss:0.30749534936981965\n",
      "eval info: group_auc:0.622, mean_mrr:0.2844, ndcg@10:0.3747, ndcg@5:0.3097\n",
      "at epoch 21 , train time: 182.4 eval time: 22.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:02,  5.94it/s]\n",
      "586it [00:01, 452.06it/s]\n",
      "236it [00:09, 23.94it/s]\n",
      "7538it [00:02, 2648.04it/s]\n",
      "1it [00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 22\n",
      "train info: logloss loss:0.28618806476051306\n",
      "eval info: group_auc:0.6202, mean_mrr:0.2859, ndcg@10:0.3739, ndcg@5:0.3105\n",
      "at epoch 22 , train time: 182.7 eval time: 23.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.88it/s]\n",
      "586it [00:01, 372.02it/s]\n",
      "236it [00:09, 24.13it/s]\n",
      "7538it [00:03, 2426.30it/s]\n",
      "1it [00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 23\n",
      "train info: logloss loss:0.2790450444084834\n",
      "eval info: group_auc:0.6172, mean_mrr:0.2826, ndcg@10:0.3697, ndcg@5:0.3078\n",
      "at epoch 23 , train time: 184.7 eval time: 24.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.87it/s]\n",
      "586it [00:01, 346.72it/s]\n",
      "236it [00:10, 23.13it/s]\n",
      "7538it [00:03, 2048.00it/s]\n",
      "1it [00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 24\n",
      "train info: logloss loss:0.26224854398813807\n",
      "eval info: group_auc:0.6184, mean_mrr:0.2824, ndcg@10:0.3708, ndcg@5:0.3062\n",
      "at epoch 24 , train time: 185.0 eval time: 25.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.88it/s]\n",
      "586it [00:01, 346.20it/s]\n",
      "236it [00:10, 23.36it/s]\n",
      "7538it [00:03, 2016.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 25\n",
      "train info: logloss loss:0.2517746432373921\n",
      "eval info: group_auc:0.6149, mean_mrr:0.2824, ndcg@10:0.3703, ndcg@5:0.3058\n",
      "at epoch 25 , train time: 184.6 eval time: 26.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.86it/s]\n",
      "586it [00:01, 356.53it/s]\n",
      "236it [00:10, 23.29it/s]\n",
      "7538it [00:04, 1724.07it/s]\n",
      "1it [00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 26\n",
      "train info: logloss loss:0.24468194278948435\n",
      "eval info: group_auc:0.6259, mean_mrr:0.2885, ndcg@10:0.3778, ndcg@5:0.3142\n",
      "at epoch 26 , train time: 185.3 eval time: 26.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:06,  5.84it/s]\n",
      "586it [00:01, 357.63it/s]\n",
      "236it [00:09, 23.88it/s]\n",
      "7538it [00:04, 1650.18it/s]\n",
      "1it [00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 27\n",
      "train info: logloss loss:0.22973876987613093\n",
      "eval info: group_auc:0.6197, mean_mrr:0.2858, ndcg@10:0.3741, ndcg@5:0.3108\n",
      "at epoch 27 , train time: 186.1 eval time: 26.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:06,  5.83it/s]\n",
      "586it [00:01, 360.35it/s]\n",
      "236it [00:10, 23.37it/s]\n",
      "7538it [00:03, 2025.21it/s]\n",
      "1it [00:00,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 28\n",
      "train info: logloss loss:0.22270132720806596\n",
      "eval info: group_auc:0.6231, mean_mrr:0.2882, ndcg@10:0.3773, ndcg@5:0.3138\n",
      "at epoch 28 , train time: 186.2 eval time: 26.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.85it/s]\n",
      "586it [00:01, 331.22it/s]\n",
      "236it [00:09, 24.04it/s]\n",
      "7538it [00:02, 2978.77it/s]\n",
      "1it [00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 29\n",
      "train info: logloss loss:0.21212992040032236\n",
      "eval info: group_auc:0.6204, mean_mrr:0.2868, ndcg@10:0.3753, ndcg@5:0.3117\n",
      "at epoch 29 , train time: 185.7 eval time: 22.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.89it/s]\n",
      "586it [00:01, 351.47it/s]\n",
      "236it [00:10, 23.47it/s]\n",
      "7538it [00:03, 1967.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 30\n",
      "train info: logloss loss:0.2071587186860706\n",
      "eval info: group_auc:0.6181, mean_mrr:0.2829, ndcg@10:0.3708, ndcg@5:0.3074\n",
      "at epoch 30 , train time: 184.3 eval time: 25.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.89it/s]\n",
      "586it [00:01, 389.35it/s]\n",
      "236it [00:10, 23.48it/s]\n",
      "7538it [00:03, 1909.60it/s]\n",
      "1it [00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 31\n",
      "train info: logloss loss:0.20428568221848203\n",
      "eval info: group_auc:0.6187, mean_mrr:0.2863, ndcg@10:0.3736, ndcg@5:0.3118\n",
      "at epoch 31 , train time: 184.4 eval time: 26.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.89it/s]\n",
      "586it [00:01, 381.83it/s]\n",
      "236it [00:10, 23.40it/s]\n",
      "7538it [00:03, 1944.24it/s]\n",
      "1it [00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 32\n",
      "train info: logloss loss:0.19049582071602345\n",
      "eval info: group_auc:0.6225, mean_mrr:0.2882, ndcg@10:0.3764, ndcg@5:0.3136\n",
      "at epoch 32 , train time: 184.5 eval time: 26.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.87it/s]\n",
      "586it [00:01, 397.72it/s]\n",
      "236it [00:10, 23.49it/s]\n",
      "7538it [00:03, 2055.53it/s]\n",
      "1it [00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 33\n",
      "train info: logloss loss:0.18443211014030936\n",
      "eval info: group_auc:0.6171, mean_mrr:0.2829, ndcg@10:0.3705, ndcg@5:0.3074\n",
      "at epoch 33 , train time: 185.1 eval time: 25.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.86it/s]\n",
      "586it [00:01, 382.01it/s]\n",
      "236it [00:09, 23.72it/s]\n",
      "7538it [00:04, 1621.58it/s]\n",
      "1it [00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 34\n",
      "train info: logloss loss:0.18468079963205022\n",
      "eval info: group_auc:0.6182, mean_mrr:0.2854, ndcg@10:0.372, ndcg@5:0.3112\n",
      "at epoch 34 , train time: 185.2 eval time: 27.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.87it/s]\n",
      "586it [00:01, 395.24it/s]\n",
      "236it [00:09, 24.08it/s]\n",
      "7538it [00:02, 2761.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 35\n",
      "train info: logloss loss:0.1782173028251784\n",
      "eval info: group_auc:0.6211, mean_mrr:0.2866, ndcg@10:0.3743, ndcg@5:0.3129\n",
      "at epoch 35 , train time: 185.2 eval time: 24.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.92it/s]\n",
      "586it [00:01, 380.34it/s]\n",
      "236it [00:10, 23.47it/s]\n",
      "7538it [00:03, 1922.11it/s]\n",
      "1it [00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 36\n",
      "train info: logloss loss:0.17686548285807172\n",
      "eval info: group_auc:0.6197, mean_mrr:0.2843, ndcg@10:0.3732, ndcg@5:0.3081\n",
      "at epoch 36 , train time: 183.5 eval time: 26.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.91it/s]\n",
      "586it [00:01, 353.43it/s]\n",
      "236it [00:09, 23.91it/s]\n",
      "7538it [00:03, 2422.31it/s]\n",
      "1it [00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 37\n",
      "train info: logloss loss:0.1688809787261113\n",
      "eval info: group_auc:0.617, mean_mrr:0.2822, ndcg@10:0.3698, ndcg@5:0.3091\n",
      "at epoch 37 , train time: 183.9 eval time: 24.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.85it/s]\n",
      "586it [00:01, 351.15it/s]\n",
      "236it [00:10, 23.24it/s]\n",
      "7538it [00:04, 1684.00it/s]\n",
      "1it [00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 38\n",
      "train info: logloss loss:0.1691346126625702\n",
      "eval info: group_auc:0.6164, mean_mrr:0.2824, ndcg@10:0.3709, ndcg@5:0.3067\n",
      "at epoch 38 , train time: 185.7 eval time: 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:06,  5.82it/s]\n",
      "586it [00:01, 368.56it/s]\n",
      "236it [00:10, 23.26it/s]\n",
      "7538it [00:04, 1790.62it/s]\n",
      "1it [00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 39\n",
      "train info: logloss loss:0.1651448489657327\n",
      "eval info: group_auc:0.6203, mean_mrr:0.2879, ndcg@10:0.3751, ndcg@5:0.3126\n",
      "at epoch 39 , train time: 186.8 eval time: 25.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:05,  5.87it/s]\n",
      "586it [00:01, 340.10it/s]\n",
      "236it [00:09, 23.67it/s]\n",
      "7538it [00:03, 2336.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 40\n",
      "train info: logloss loss:0.16401585997262383\n",
      "eval info: group_auc:0.6174, mean_mrr:0.283, ndcg@10:0.3699, ndcg@5:0.3071\n",
      "at epoch 40 , train time: 185.1 eval time: 24.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:02,  5.97it/s]\n",
      "586it [00:01, 426.19it/s]\n",
      "236it [00:09, 23.67it/s]\n",
      "7538it [00:03, 1980.34it/s]\n",
      "1it [00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 41\n",
      "train info: logloss loss:0.15777124081091684\n",
      "eval info: group_auc:0.6203, mean_mrr:0.2858, ndcg@10:0.3747, ndcg@5:0.3113\n",
      "at epoch 41 , train time: 182.0 eval time: 25.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:01,  5.98it/s]\n",
      "586it [00:01, 344.02it/s]\n",
      "236it [00:09, 23.88it/s]\n",
      "7538it [00:03, 2426.74it/s]\n",
      "1it [00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 42\n",
      "train info: logloss loss:0.15856884594090817\n",
      "eval info: group_auc:0.6192, mean_mrr:0.2878, ndcg@10:0.3755, ndcg@5:0.3138\n",
      "at epoch 42 , train time: 181.6 eval time: 25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.91it/s]\n",
      "586it [00:01, 342.13it/s]\n",
      "236it [00:09, 23.94it/s]\n",
      "7538it [00:03, 2106.40it/s]\n",
      "1it [00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 43\n",
      "train info: logloss loss:0.15467934830982108\n",
      "eval info: group_auc:0.6206, mean_mrr:0.2872, ndcg@10:0.3743, ndcg@5:0.3121\n",
      "at epoch 43 , train time: 183.9 eval time: 23.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.91it/s]\n",
      "586it [00:01, 342.43it/s]\n",
      "236it [00:09, 23.70it/s]\n",
      "7538it [00:03, 2457.76it/s]\n",
      "1it [00:00,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 44\n",
      "train info: logloss loss:0.1462821456797517\n",
      "eval info: group_auc:0.6176, mean_mrr:0.2854, ndcg@10:0.3728, ndcg@5:0.3099\n",
      "at epoch 44 , train time: 183.8 eval time: 23.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.92it/s]\n",
      "586it [00:01, 371.48it/s]\n",
      "236it [00:09, 23.67it/s]\n",
      "7538it [00:03, 2431.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 45\n",
      "train info: logloss loss:0.14724512847524623\n",
      "eval info: group_auc:0.6168, mean_mrr:0.2835, ndcg@10:0.3704, ndcg@5:0.306\n",
      "at epoch 45 , train time: 183.4 eval time: 23.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.89it/s]\n",
      "586it [00:01, 351.94it/s]\n",
      "236it [00:09, 23.84it/s]\n",
      "7538it [00:02, 2857.52it/s]\n",
      "1it [00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 46\n",
      "train info: logloss loss:0.1438441412895575\n",
      "eval info: group_auc:0.613, mean_mrr:0.2819, ndcg@10:0.3671, ndcg@5:0.3042\n",
      "at epoch 46 , train time: 184.2 eval time: 24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.91it/s]\n",
      "586it [00:01, 354.01it/s]\n",
      "236it [00:09, 23.89it/s]\n",
      "7538it [00:03, 2451.27it/s]\n",
      "1it [00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 47\n",
      "train info: logloss loss:0.14403545845005813\n",
      "eval info: group_auc:0.619, mean_mrr:0.2851, ndcg@10:0.373, ndcg@5:0.3088\n",
      "at epoch 47 , train time: 183.9 eval time: 23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:04,  5.90it/s]\n",
      "586it [00:01, 358.19it/s]\n",
      "236it [00:09, 24.05it/s]\n",
      "7538it [00:03, 2300.31it/s]\n",
      "1it [00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 48\n",
      "train info: logloss loss:0.14103643729910598\n",
      "eval info: group_auc:0.6136, mean_mrr:0.2817, ndcg@10:0.3679, ndcg@5:0.3048\n",
      "at epoch 48 , train time: 184.1 eval time: 25.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.92it/s]\n",
      "586it [00:01, 384.29it/s]\n",
      "236it [00:09, 23.88it/s]\n",
      "7538it [00:03, 2083.72it/s]\n",
      "1it [00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 49\n",
      "train info: logloss loss:0.1400783404040718\n",
      "eval info: group_auc:0.6138, mean_mrr:0.2801, ndcg@10:0.3669, ndcg@5:0.3026\n",
      "at epoch 49 , train time: 183.4 eval time: 25.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [03:03,  5.92it/s]\n",
      "586it [00:01, 361.17it/s]\n",
      "236it [00:09, 23.82it/s]\n",
      "7538it [00:03, 2429.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 50\n",
      "train info: logloss loss:0.14093992197423022\n",
      "eval info: group_auc:0.6108, mean_mrr:0.2798, ndcg@10:0.3654, ndcg@5:0.3026\n",
      "at epoch 50 , train time: 183.3 eval time: 25.0\n",
      "CPU times: user 5h 7min 30s, sys: 1h 12min 13s, total: 6h 19min 43s\n",
      "Wall time: 2h 52min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_results=model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586it [00:01, 352.55it/s]\n",
      "236it [00:09, 23.74it/s]\n",
      "7538it [00:03, 2280.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6108, 'mean_mrr': 0.2798, 'ndcg@5': 0.3026, 'ndcg@10': 0.3654}\n",
      "CPU times: user 1min 13s, sys: 1min 1s, total: 2min 15s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstur_loss_lr3e-4.txt', 'w') as filehandle:\n",
    "    for listitem in train_results:\n",
    "        for item in listitem:\n",
    "            filehandle.write('%s\\n' % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model_newlr\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"lstur_lr3e-4_50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
